---
title: "MovieLens Project"
author: "Kunjal"
date: "12/22/2021"
output: 
  pdf_document:
    toc: true
    toc_depth: 4
    number_sections: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Background and Motivation overview

A recommendation system is a subclass of information filtering system that seeks to predict the "rating" or 
"preference" a user would give to an item. In this project the items are movies. Recommendation systems are one of the most used models in machine learning algorithms. 

Many companies are using these recommendation systems in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general.  A strong recommendation system was of such importance that in 2006, Netflix offered a million dollar prize to anyone who could improve the effectiveness of its recommendation system by 10%.

## Introduction

This project is related to the MovieLens Project of the HarvardX: PH125.9x Data Science: Capstone course. For this project we will focus on create a movie recommendation system using the 10M version of MovieLens dataset, collected by GroupLens Research.

The present report start with a general idea of the project. Then the given dataset will be prepared and setup. An exploratory data analysis is carried out in order to develop a machine learning algorithm that could predict movie ratings until a final model. Results will be explained. Finally the report ends with some concluding remarks. 
 
## Aim of the project

The aim of this project is to train a machine learning algorithm using the inputs of a provided training subset to predict movie ratings in a validation set.


The value used to evaluate algorithm performance is the Root Mean Square Error, or RMSE. RMSE is one of the most used measure of the differences between values predicted by a model and the values observed. RMSE is a measure of accuracy, to compare forecasting errors of different models for a particular dataset, a lower RMSE is better than a higher one. 

## Data Ingestion

For this project a movie rating predictor is created using the 'MovieLens' dataset. This data set can be found and downloaded here:

• [MovieLens 10M dataset] https://grouplens.org/datasets/movielens/10m/

• [MovieLens 10M dataset - zip file] http://files.grouplens.org/datasets/movielens/ml-10m.zip

The below chunk of code gives a partition of the data set for training and testing our data. It also removes the unnecessary files from the working directory.



   
```{r }

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
Algorithm development is to be carried out on the "edx" subset only, as "validation" subset will be used to test the final algorithm.

We are going to use additional libraries:

```{r loading-libs, message=FALSE}
library(ggplot2)
library(lubridate)
```
\pagebreak

# Methods and Analysis

##  Data Pre Processing

We modify the columns to suitable formats that can be further used for analysis.
  
```{r }
# Modify the year as a column in the both datasets
edx <- edx %>% mutate(year = as.numeric(str_sub(title,-5,-2)))
validation <- validation %>% mutate(year = as.numeric(str_sub(title,-5,-2)))
```



## Exploratory Analysis

To get familiar with the dataset, we find that there are six variables "userId","movieID","rating","timestamp",
"title","genres" in the subset. Each row represent a single rating of a user for a single movie.

```{r }
head(edx) 
```

 A Summary of the subset confirms that there are no missing values.
 
```{r }
summary(edx)
```


The total of unique movies and users in the edx subset is given in the below chunk of code.

```{r }
# Number of unique movies and users in the edx dataset 
edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```

The popularity of the movie genre depends strongly on the contemporary issues. The below code shows number of movie ratings for certain genres.

```{r }  
  #Movie ratings are in each of the following genres in the edx dataset
genres = c("Drama", "Comedy", "Thriller", "Romance")
sapply(genres, function(g) {
  sum(str_detect(edx$genres, g))
})
```


A summary statistics of rating in edx subset.The 4 is the most common rating, followed by 3 and 0.5 is the least common rating.

```{r }
summary(edx$rating)
edx %>% group_by(title)%>%summarise(number = n())%>%arrange(desc(number))
#The five most given ratings in order from most to least
head(sort(-table(edx$rating)),5)
```
Users have a preference to rate movies rather higher than lower as shown by the distribution of ratings below. 4 is the most common rating, followed by 3 and 5. 0.5 is the least common rating. In general, half rating are less common than whole star ratings.

```{r echo=FALSE}
#Plot of rating 
# Ratings distribution

table(edx$rating)
edx %>%
  group_by(rating) %>%
  summarize(count = n()) %>%
  ggplot(aes(x = rating, y = count)) +
  geom_line(color = "blue")


# Ratings Histogram
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, color = "blue") +
  xlab("Rating") +
  ylab("Count") +
  ggtitle("Rating distribution") +
  theme(plot.title = element_text(hjust = 0.5)) 

```

From the above plot, half star ratings are less common than whole star ratings. The average rating for each year is shown in below.
   
```{r }
#Average ratings of edx dataset
avg_ratings <- edx %>% group_by(year) %>% summarise(avg_rating = mean(rating)) 
avg_ratings

```



## Data Analysis Strategies

We can observe that some movies have been rated more often than other, while some have very few  ratings and sometimes only one rating. This will be important for our model as very low rating numbers might results in untrustworthy estimate for our predictions. In fact 125 movies have been rated only once. 

Thus regularization and a penalty term will be applied to the models in this project. Regularization are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid over fitting (the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably). Regularization is a technique used for tuning the function by adding an additional penalty term in the error function. The additional term controls the excessively fluctuating function such that the coefficients don’t take extreme values.


This is called movie bias. The distribution of movie bias effect (b_i) is given below.


```{r,echo=FALSE}
edx %>%
  count(movieId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "blue") +
  scale_x_log10() +
  xlab("Number of ratings") +
  ylab("Number of movies") +
  ggtitle("Number of ratings per movie")
```





We can observe that the majority of users have rated between 30 and 100 movies. So, a user penalty term need to be included later in our models.

Some users are positive and some have negative reviews because of their own personal liking/disliking regardless of movie.The distribution of user bias effect (b_u) is given below.

```{r,echo=FALSE}
edx %>%
  count(userId) %>%
  ggplot(aes(n)) +
  geom_histogram(bins = 30, color = "blue") +
  scale_x_log10() +
  xlab("Number of ratings") + 
  ylab("Number of users") +
  ggtitle("Number of ratings given by users")
```
Estimating the trend of rating versus release year-Year Effect.The general trend shows modern users relatively rate movies lower.  The users mindset also evolve over time.This can also effect the average rating of movies over the years.The plot of year bias effect(b_y)is given below.

```{r, echo=FALSE}


edx %>% group_by(year) %>%
  summarize(Rating = mean(rating)) %>%
  ggplot(aes(year, Rating)) +
  geom_point() +
  geom_smooth()+ 
  ggtitle("Rating vs Release year trend")
```


As 20 movies that were rated only once appear to be obscure, predictions of future ratings for them will be difficult.


```{r, echo=FALSE}
edx %>%
  group_by(movieId) %>%
  summarize(count = n()) %>%
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
  
```

Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.


```{r, echo=FALSE}
edx %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>%
  ggplot(aes(b_u)) +
  geom_histogram(bins = 30, color = "blue") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
  
```


## Modelling Approach


The value used to evaluate algorithm performance is the Root Mean Square Error(RMSE). RMSE is one of the most used measure of the differences between values predicted by a model and the values observed. RMSE is a measure of accuracy, lower the RMSE is better than higher one. The effect of each error on RMSE is proportional to the size of the squared error; thus larger errors will have large effect on RMSE. RMSE is sensitive to outliers. The evaluation criteria for this algorithm is a RMSE expected to be lower than 0.8775.
  
```{r }
#Root Mean Square Error Loss Function
#Function that computes the RMSE for vectors of ratings and their corresponding predictors
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings-predicted_ratings)^2,na.rm=T))
}
```



### I. Average movie rating model

The first basic model predicts the same rating for all movies, so we compute the dataset’s mean rating. The expected rating of the underlying data set is between 3 and 4.


We start by building the simplest possible recommender system by predicting the same rating for all movies regardless of user who give it. A model based approach assumes the same rating for all movie with all differences explained by random variation :
$$ Y_{u, i} = \mu + \epsilon_{u, i} $$

with $\epsilon_{u,i}$ independent error sample from the same distribution centered at 0 and $\mu$ the “true” rating for all movies. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone. We know that the estimate that minimize the RMSE is the least square estimate of $Y_{u,i}$ , in this case, is the average of all ratings:


The expected rating of the underlying data set is between 3 and 4.

```{r, echo = TRUE}
mu <- mean(edx$rating)
mu
```


If we predict all unknown ratings with $\mu$ or mu, we obtain the first naive RMSE:

```{r, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mu)
naive_rmse
```


Here, we represent results table with the first RMSE:

```{r, echo = TRUE}
rmse_results <- data_frame(method = "Average movie rating model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

This give us our baseline RMSE to compare with next modelling approaches.

In order to do better than simply predicting the average rating, we incorporate some of insights we gained during the exploratory data analysis.


### II.  Movie effect model

To improve above model we focus on the fact that, from experience, we know that some movies are just generally rated higher than others. Higher ratings are mostly linked to popular movies among users and the opposite is true for unpopular movies. We compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The histogram is left skewed, implying that more movies have negative effects


```{r, echo=FALSE}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 20, data = ., color = I("blue"),
                     ylab = "Number of movies", main = "Movie effect model - Penalty Term (b_i)")
```


This is called the penalty term movie effect.

Our prediction improve once we predict using this model.

```{r, echo=FALSE}
predicted_ratings <- mu +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_1_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model - Penalty Term (b_i)",  
                                     RMSE = model_1_rmse ))
rmse_results %>% knitr::kable()
```


So we have predicted movie rating based on the fact that movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse that the average rating of all movies $\mu$ , we predict that it will rated lower that $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.

We can see an improvement but this model does not consider the individual user rating effect.


### III. Movie and user effect model


We compute the average rating for user $\mu$, for those that have rated over 100 movies, said penalty term user effect. In fact users affect the ratings positively or negatively.

```{r, echo=FALSE}
user_avgs<- edx %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating - mu - b_i))
user_avgs%>% qplot(b_u, geom ="histogram", bins = 20, data = ., color = I("blue"),
                   ylab = "Number of movies", main = "Movie and user effect model - Penalty Term (b_u)")
```

There is substantial variability across users as well: some users are very cranky and other love every movie. This implies that further improvement to our model my be:
$$Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}$$

where $b_{u}$ is a user-specific effect. If a cranky user (negative $b_{u}$ rates a great movie (positive $b_{i}$), the effects counter each other and we may be able to correctly predict that this user gave this great movie a 3 rather than a 5.


We compute an approximation by computing $\mu$ and $b_{i}$, and estimating  $b_{u}$, as the average of $$Y_{u, i} - \mu - b_{i}$$


```{r, echo = TRUE}
user_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
  
```

We can now construct predictors and see RMSE improves:


```{r, echo = TRUE}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_2_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie and user effect model - Penalty Term (b_u)",  
                                     RMSE = model_2_rmse))
rmse_results %>% knitr::kable()
```


Our rating predictions further reduced the RMSE. But we made stil mistakes on our first model (using only movies). The supposes “best “ and “worst “movie were rated by few users, in most cases just one user. These movies were mostly obscure ones. This is because with a few users, we have more uncertainty. Therefore larger estimates of $b_{i}$, negative or positive, are more likely.


Large errors can increase our RMSE. 


### IV Movie, user and year effect model

Our rating predictions further reduced the RMSE. Year of the movie also effect ratings.  

```{r, echo=FALSE}
year_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(year) %>%
  summarize(b_y = mean(rating - mu - b_i - b_u))
```
```{r}
predicted_ratings <- validation%>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(year_avgs, by='year') %>%
  mutate(pred = mu + b_i + b_u + b_y) %>%
  pull(pred)
model_3_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie, User and Year effect model - Penalty Term (b_y)",  
                                     RMSE = model_3_rmse))
rmse_results %>% knitr::kable()
```


### V. Regularized movie, user and year effect model

So estimates of $b_{i}$, $b_{u}$  and $b_{y}$ are caused by movies with very few ratings and in some users that only rated a very small number of movies, and production year Hence this can strongly influence the prediction. The use of the regularization permits to penalize these aspects. We should find the value of lambda (that is a tuning parameter) that will minimize the RMSE. This shrinks the $b_{i}$, $b_{u}$ and $b_{y}$ in case of small number of ratings.


```{r, echo = TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mu <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+l))
  
  b_y <- edx %>%
    left_join(b_i, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(year) %>%
    summarize(b_y = sum(rating - mu - b_i - b_u)/(n()+l), n_y = n())
   
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})
```


We plot RMSE vs lambdas to select the optimal lambda

```{r, echo = TRUE}
qplot(lambdas, rmses)  
```

For the full model, the optimal lambda is:

```{r , echo = TRUE}
  lambda <- lambdas[which.min(rmses)]
lambda
```

For the full model, the optimal lambda is: 5. A lambda of 5 was shown to be the best Tune for the train and test sets of this Regularized Model as well and therefore will be used on the validation set.

The new results will be:


```{r, echo = TRUE}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized movie, user and year effect model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

\pagebreak

# Results

The RMSE values of all the represented models are the following:

```{r, echo = FALSE}
rmse_results %>% knitr::kable()
```

We therefore found the lowest value of RMSE that is 0.8645218.

The lowest RMSE using the validation set is the Final Validation Model featuring Regularized Movie, user and year Effects. It significantly improves upon the Benchmarking Model’s RMSE of 1.06.

# Discussion

This model work well if the average user doesn't rate a particularly good/popular movie with a large positive $b_{i}$, by disliking a particular movie. and if the user don't rate the movie by thier production year.


# Conclusion

We can affirm to have built a machine learning algorithm to predict movie ratings with MovieLens dataset.
The regularized model including the effect of user is characterized by the lower RMSE value and is hence the optimal model to use for the present project.

The optimal model characterised by the lowest RMSE value (0.8645218) lower than the initial evaluation criteria (0.8775) given by the goal of the present project.



\pagebreak

# Appendix - Enviroment

```{r}
print("Operating System:")
version
```



